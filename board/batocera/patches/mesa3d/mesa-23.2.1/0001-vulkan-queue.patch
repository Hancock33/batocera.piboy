diff --git a/src/vulkan/runtime/vk_queue.c b/src/vulkan/runtime/vk_queue.c
index cacca2f9b2c..5cdfa36580a 100644
--- a/src/vulkan/runtime/vk_queue.c
+++ b/src/vulkan/runtime/vk_queue.c
@@ -570,12 +570,6 @@ vk_queue_enable_submit_thread(struct vk_queue *queue)
 struct vulkan_submit_info {
    const void *pNext;
 
-   uint32_t command_buffer_count;
-   const VkCommandBufferSubmitInfo *command_buffers;
-
-   uint32_t wait_count;
-   const VkSemaphoreSubmitInfo *waits;
-
    uint32_t signal_count;
    const VkSemaphoreSubmitInfo *signals;
 
@@ -591,60 +585,17 @@ struct vulkan_submit_info {
    struct vk_fence *fence;
 };
 
-static VkResult
-vk_queue_submit(struct vk_queue *queue,
-                const struct vulkan_submit_info *info)
+static bool
+vk_queue_parse_waits(struct vk_device *device,
+                     uint32_t wait_count,
+                     const VkSemaphoreSubmitInfo *waits,
+                     uint32_t offset,
+                     struct vk_queue_submit *submit)
 {
-   struct vk_device *device = queue->base.device;
-   VkResult result;
-   uint32_t sparse_memory_bind_entry_count = 0;
-   uint32_t sparse_memory_image_bind_entry_count = 0;
-   VkSparseMemoryBind *sparse_memory_bind_entries = NULL;
-   VkSparseImageMemoryBind *sparse_memory_image_bind_entries = NULL;
-
-   for (uint32_t i = 0; i < info->buffer_bind_count; ++i)
-      sparse_memory_bind_entry_count += info->buffer_binds[i].bindCount;
-
-   for (uint32_t i = 0; i < info->image_opaque_bind_count; ++i)
-      sparse_memory_bind_entry_count += info->image_opaque_binds[i].bindCount;
-
-   for (uint32_t i = 0; i < info->image_bind_count; ++i)
-      sparse_memory_image_bind_entry_count += info->image_binds[i].bindCount;
-
-   const struct wsi_memory_signal_submit_info *mem_signal =
-      vk_find_struct_const(info->pNext, WSI_MEMORY_SIGNAL_SUBMIT_INFO_MESA);
-   bool signal_mem_sync = mem_signal != NULL &&
-                          mem_signal->memory != VK_NULL_HANDLE &&
-                          queue->base.device->create_sync_for_memory != NULL;
-
-   struct vk_queue_submit *submit =
-      vk_queue_submit_alloc(queue, info->wait_count,
-                            info->command_buffer_count,
-                            info->buffer_bind_count,
-                            info->image_opaque_bind_count,
-                            info->image_bind_count,
-                            sparse_memory_bind_entry_count,
-                            sparse_memory_image_bind_entry_count,
-                            info->signal_count +
-                            signal_mem_sync + (info->fence != NULL),
-                            &sparse_memory_bind_entries,
-                            &sparse_memory_image_bind_entries);
-   if (unlikely(submit == NULL))
-      return vk_error(queue, VK_ERROR_OUT_OF_HOST_MEMORY);
-
-   /* From the Vulkan 1.2.194 spec:
-    *
-    *    "If the VkSubmitInfo::pNext chain does not include this structure,
-    *    the batch defaults to use counter pass index 0."
-    */
-   const VkPerformanceQuerySubmitInfoKHR *perf_info =
-      vk_find_struct_const(info->pNext, PERFORMANCE_QUERY_SUBMIT_INFO_KHR);
-   submit->perf_pass_index = perf_info ? perf_info->counterPassIndex : 0;
-
    bool has_binary_permanent_semaphore_wait = false;
-   for (uint32_t i = 0; i < info->wait_count; i++) {
+   for (uint32_t i = 0; i < wait_count; i++) {
       VK_FROM_HANDLE(vk_semaphore, semaphore,
-                     info->waits[i].semaphore);
+                     waits[i].semaphore);
 
       /* From the Vulkan 1.2.194 spec:
        *
@@ -667,7 +618,7 @@ vk_queue_submit(struct vk_queue *queue,
       struct vk_sync *sync;
       if (semaphore->temporary) {
          assert(semaphore->type == VK_SEMAPHORE_TYPE_BINARY);
-         sync = submit->_wait_temps[i] = semaphore->temporary;
+         sync = submit->_wait_temps[i + offset] = semaphore->temporary;
          semaphore->temporary = NULL;
       } else {
          if (semaphore->type == VK_SEMAPHORE_TYPE_BINARY) {
@@ -680,20 +631,29 @@ vk_queue_submit(struct vk_queue *queue,
       }
 
       uint64_t wait_value = semaphore->type == VK_SEMAPHORE_TYPE_TIMELINE ?
-                            info->waits[i].value : 0;
+                            waits[i].value : 0;
 
-      submit->waits[i] = (struct vk_sync_wait) {
+      submit->waits[i + offset] = (struct vk_sync_wait) {
          .sync = sync,
-         .stage_mask = info->waits[i].stageMask,
+         .stage_mask = waits[i].stageMask,
          .wait_value = wait_value,
       };
    }
+   return has_binary_permanent_semaphore_wait;
+}
 
-   for (uint32_t i = 0; i < info->command_buffer_count; i++) {
+static void
+vk_queue_parse_cmdbufs(struct vk_queue *queue,
+                       uint32_t command_buffer_count,
+                       const VkCommandBufferSubmitInfo *command_buffers,
+                       uint32_t offset,
+                       struct vk_queue_submit *submit)
+{
+   for (uint32_t i = 0; i < command_buffer_count; i++) {
       VK_FROM_HANDLE(vk_command_buffer, cmd_buffer,
-                     info->command_buffers[i].commandBuffer);
-      assert(info->command_buffers[i].deviceMask == 0 ||
-             info->command_buffers[i].deviceMask == 1);
+                     command_buffers[i].commandBuffer);
+      assert(command_buffers[i].deviceMask == 0 ||
+             command_buffers[i].deviceMask == 1);
       assert(cmd_buffer->pool->queue_family_index == queue->queue_family_index);
 
       /* Some drivers don't call vk_command_buffer_begin/end() yet and, for
@@ -705,11 +665,120 @@ vk_queue_submit(struct vk_queue *queue,
              cmd_buffer->state == MESA_VK_COMMAND_BUFFER_STATE_PENDING);
       cmd_buffer->state = MESA_VK_COMMAND_BUFFER_STATE_PENDING;
 
-      submit->command_buffers[i] = cmd_buffer;
+      submit->command_buffers[i + offset] = cmd_buffer;
+   }
+}
+
+static VkResult
+vk_queue_handle_threaded_waits(struct vk_queue *queue,
+                               uint32_t wait_count,
+                               const VkSemaphoreSubmitInfo *waits,
+                               unsigned offset,
+                               struct vk_queue_submit *submit)
+{
+   assert(queue->submit.mode == VK_QUEUE_SUBMIT_MODE_THREADED);
+   for (uint32_t i = 0; i < wait_count; i++) {
+      VK_FROM_HANDLE(vk_semaphore, semaphore,
+                     waits[i].semaphore);
+
+      if (semaphore->type != VK_SEMAPHORE_TYPE_BINARY)
+         continue;
+
+      /* From the Vulkan 1.2.194 spec:
+         *
+         *    "When a batch is submitted to a queue via a queue
+         *    submission, and it includes semaphores to be waited on,
+         *    it defines a memory dependency between prior semaphore
+         *    signal operations and the batch, and defines semaphore
+         *    wait operations.
+         *
+         *    Such semaphore wait operations set the semaphores
+         *    created with a VkSemaphoreType of
+         *    VK_SEMAPHORE_TYPE_BINARY to the unsignaled state."
+         *
+         * For threaded submit, we depend on tracking the unsignaled
+         * state of binary semaphores to determine when we can safely
+         * submit.  The VK_SYNC_WAIT_PENDING check above as well as the
+         * one in the sumbit thread depend on all binary semaphores
+         * being reset when they're not in active use from the point
+         * of view of the client's CPU timeline.  This means we need to
+         * reset them inside vkQueueSubmit and cannot wait until the
+         * actual submit which happens later in the thread.
+         *
+         * We've already stolen temporary semaphore payloads above as
+         * part of basic semaphore processing.  We steal permanent
+         * semaphore payloads here by way of vk_sync_move.  For shared
+         * semaphores, this can be a bit expensive (sync file import
+         * and export) but, for non-shared semaphores, it can be made
+         * fairly cheap.  Also, we only do this semaphore swapping in
+         * the case where you have real timelines AND the client is
+         * using timeline semaphores with wait-before-signal (that's
+         * the only way to get a submit thread) AND mixing those with
+         * waits on binary semaphores AND said binary semaphore is
+         * using its permanent payload.  In other words, this code
+         * should basically only ever get executed in CTS tests.
+         */
+      if (submit->_wait_temps[i + offset] != NULL)
+         continue;
+
+      assert(submit->waits[i + offset].sync == &semaphore->permanent);
+
+      /* From the Vulkan 1.2.194 spec:
+         *
+         *    VUID-vkQueueSubmit-pWaitSemaphores-03238
+         *
+         *    "All elements of the pWaitSemaphores member of all
+         *    elements of pSubmits created with a VkSemaphoreType of
+         *    VK_SEMAPHORE_TYPE_BINARY must reference a semaphore
+         *    signal operation that has been submitted for execution
+         *    and any semaphore signal operations on which it depends
+         *    (if any) must have also been submitted for execution."
+         *
+         * Therefore, we can safely do a blocking wait here and it
+         * won't actually block for long.  This ensures that the
+         * vk_sync_move below will succeed.
+         */
+      VkResult result = vk_sync_wait(queue->base.device,
+                                     submit->waits[i].sync, 0,
+                                     VK_SYNC_WAIT_PENDING, UINT64_MAX);
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+
+      result = vk_sync_create(queue->base.device,
+                              semaphore->permanent.type,
+                              0 /* flags */,
+                              0 /* initial value */,
+                              &submit->_wait_temps[i + offset]);
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+
+      result = vk_sync_move(queue->base.device,
+                              submit->_wait_temps[i + offset],
+                              &semaphore->permanent);
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+
+      submit->waits[i + offset].sync = submit->_wait_temps[i + offset];
    }
+   return VK_SUCCESS;
+}
 
-   sparse_memory_bind_entry_count = 0;
-   sparse_memory_image_bind_entry_count = 0;
+static VkResult
+vk_queue_submit(struct vk_queue *queue,
+                const struct vulkan_submit_info *info,
+                struct vk_queue_submit *submit,
+                uint32_t perf_pass_index,
+                struct vk_sync *mem_sync,
+                bool has_binary_permanent_semaphore_wait,
+                VkSparseMemoryBind *sparse_memory_bind_entries,
+                VkSparseImageMemoryBind *sparse_memory_image_bind_entries)
+{
+   struct vk_device *device = queue->base.device;
+   VkResult result;
+   uint32_t sparse_memory_bind_entry_count = 0;
+   uint32_t sparse_memory_image_bind_entry_count = 0;
+
+   submit->perf_pass_index = perf_pass_index;
 
    if (info->buffer_binds)
       typed_memcpy(submit->buffer_binds, info->buffer_binds, info->buffer_bind_count);
@@ -797,14 +866,7 @@ vk_queue_submit(struct vk_queue *queue,
    }
 
    uint32_t signal_count = info->signal_count;
-   if (signal_mem_sync) {
-      struct vk_sync *mem_sync;
-      result = queue->base.device->create_sync_for_memory(queue->base.device,
-                                                          mem_signal->memory,
-                                                          true, &mem_sync);
-      if (unlikely(result != VK_SUCCESS))
-         goto fail;
-
+   if (mem_sync) {
       submit->_mem_signal_temp = mem_sync;
 
       assert(submit->signals[signal_count].sync == NULL);
@@ -893,95 +955,9 @@ vk_queue_submit(struct vk_queue *queue,
       return vk_device_flush(queue->base.device);
 
    case VK_QUEUE_SUBMIT_MODE_THREADED:
-      if (has_binary_permanent_semaphore_wait) {
-         for (uint32_t i = 0; i < info->wait_count; i++) {
-            VK_FROM_HANDLE(vk_semaphore, semaphore,
-                           info->waits[i].semaphore);
-
-            if (semaphore->type != VK_SEMAPHORE_TYPE_BINARY)
-               continue;
-
-            /* From the Vulkan 1.2.194 spec:
-             *
-             *    "When a batch is submitted to a queue via a queue
-             *    submission, and it includes semaphores to be waited on,
-             *    it defines a memory dependency between prior semaphore
-             *    signal operations and the batch, and defines semaphore
-             *    wait operations.
-             *
-             *    Such semaphore wait operations set the semaphores
-             *    created with a VkSemaphoreType of
-             *    VK_SEMAPHORE_TYPE_BINARY to the unsignaled state."
-             *
-             * For threaded submit, we depend on tracking the unsignaled
-             * state of binary semaphores to determine when we can safely
-             * submit.  The VK_SYNC_WAIT_PENDING check above as well as the
-             * one in the sumbit thread depend on all binary semaphores
-             * being reset when they're not in active use from the point
-             * of view of the client's CPU timeline.  This means we need to
-             * reset them inside vkQueueSubmit and cannot wait until the
-             * actual submit which happens later in the thread.
-             *
-             * We've already stolen temporary semaphore payloads above as
-             * part of basic semaphore processing.  We steal permanent
-             * semaphore payloads here by way of vk_sync_move.  For shared
-             * semaphores, this can be a bit expensive (sync file import
-             * and export) but, for non-shared semaphores, it can be made
-             * fairly cheap.  Also, we only do this semaphore swapping in
-             * the case where you have real timelines AND the client is
-             * using timeline semaphores with wait-before-signal (that's
-             * the only way to get a submit thread) AND mixing those with
-             * waits on binary semaphores AND said binary semaphore is
-             * using its permanent payload.  In other words, this code
-             * should basically only ever get executed in CTS tests.
-             */
-            if (submit->_wait_temps[i] != NULL)
-               continue;
-
-            assert(submit->waits[i].sync == &semaphore->permanent);
-
-            /* From the Vulkan 1.2.194 spec:
-             *
-             *    VUID-vkQueueSubmit-pWaitSemaphores-03238
-             *
-             *    "All elements of the pWaitSemaphores member of all
-             *    elements of pSubmits created with a VkSemaphoreType of
-             *    VK_SEMAPHORE_TYPE_BINARY must reference a semaphore
-             *    signal operation that has been submitted for execution
-             *    and any semaphore signal operations on which it depends
-             *    (if any) must have also been submitted for execution."
-             *
-             * Therefore, we can safely do a blocking wait here and it
-             * won't actually block for long.  This ensures that the
-             * vk_sync_move below will succeed.
-             */
-            result = vk_sync_wait(queue->base.device,
-                                  submit->waits[i].sync, 0,
-                                  VK_SYNC_WAIT_PENDING, UINT64_MAX);
-            if (unlikely(result != VK_SUCCESS))
-               goto fail;
-
-            result = vk_sync_create(queue->base.device,
-                                    semaphore->permanent.type,
-                                    0 /* flags */,
-                                    0 /* initial value */,
-                                    &submit->_wait_temps[i]);
-            if (unlikely(result != VK_SUCCESS))
-               goto fail;
-
-            result = vk_sync_move(queue->base.device,
-                                  submit->_wait_temps[i],
-                                  &semaphore->permanent);
-            if (unlikely(result != VK_SUCCESS))
-               goto fail;
-
-            submit->waits[i].sync = submit->_wait_temps[i];
-         }
-      }
-
       vk_queue_push_submit(queue, submit);
 
-      if (signal_mem_sync) {
+      if (mem_sync) {
          /* If we're signaling a memory object, we have to ensure that
           * vkQueueSubmit does not return until the kernel submission has
           * happened.  Otherwise, we may get a race between this process
@@ -1132,6 +1108,74 @@ vk_queue_finish(struct vk_queue *queue)
    vk_object_base_finish(&queue->base);
 }
 
+static bool
+filter_pnexts(const void *pNext)
+{
+   vk_foreach_struct_const(s, pNext) {
+      switch (s->sType) {
+      /* can possibly be merged */
+      case VK_STRUCTURE_TYPE_PERFORMANCE_QUERY_SUBMIT_INFO_KHR:
+         break;
+      default:
+         return false;
+      }
+   }
+   return true;
+}
+
+static VkResult
+vk_queue_submit_flush(struct vk_queue *queue, const VkSubmitInfo2 *pSubmits, unsigned submit_count,
+                      uint32_t wait_count,
+                      uint32_t cmdbuf_count,
+                      uint32_t perf_pass_index, struct vk_sync *mem_sync, struct vk_fence *fence)
+{
+   VkResult result = VK_SUCCESS;
+   struct vulkan_submit_info info = {
+      .pNext = pSubmits->pNext,
+      .signal_count = pSubmits[submit_count - 1].signalSemaphoreInfoCount,
+      .signals = pSubmits[submit_count - 1].pSignalSemaphoreInfos,
+      .fence = fence
+   };
+
+   struct vk_queue_submit *submit =
+      vk_queue_submit_alloc(queue, wait_count,
+                              cmdbuf_count,
+                              info.buffer_bind_count,
+                              info.image_opaque_bind_count,
+                              info.image_bind_count,
+                              0,
+                              0,
+                              info.signal_count +
+                              (mem_sync != NULL) + (info.fence != NULL),
+                              NULL,
+                              NULL);
+   if (unlikely(submit == NULL))
+      return vk_error(queue, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   uint32_t wait_counter = 0;
+   uint32_t cmdbuf_counter = 0;
+   bool has_binary_permanent_semaphore_wait = false;
+   for (unsigned i = 0; i < submit_count; i++) {
+      uint32_t cur_wait_count = pSubmits[i].waitSemaphoreInfoCount;
+      const VkSemaphoreSubmitInfo *wait_semaphore_infos = pSubmits[i].pWaitSemaphoreInfos;
+      uint32_t cur_cmdbuf_count = pSubmits[i].commandBufferInfoCount;
+      const VkCommandBufferSubmitInfo *cmdbufs = pSubmits[i].pCommandBufferInfos;
+      has_binary_permanent_semaphore_wait |= vk_queue_parse_waits(queue->base.device, cur_wait_count, wait_semaphore_infos, wait_counter, submit);
+      vk_queue_parse_cmdbufs(queue, cur_cmdbuf_count, cmdbufs, cmdbuf_counter, submit);
+      if (has_binary_permanent_semaphore_wait && queue->submit.mode == VK_QUEUE_SUBMIT_MODE_THREADED) {
+         result = vk_queue_handle_threaded_waits(queue, cur_wait_count, wait_semaphore_infos, wait_counter, submit);
+         if (unlikely(result != VK_SUCCESS)) {
+            vk_queue_submit_destroy(queue, submit);
+            return result;
+         }
+      }
+      wait_counter += cur_wait_count;
+      cmdbuf_counter += cur_cmdbuf_count;
+   }
+
+   return vk_queue_submit(queue, &info, submit, perf_pass_index, mem_sync, has_binary_permanent_semaphore_wait, 0, 0);
+}
+
 VKAPI_ATTR VkResult VKAPI_CALL
 vk_common_QueueSubmit2KHR(VkQueue _queue,
                           uint32_t submitCount,
@@ -1152,20 +1196,97 @@ vk_common_QueueSubmit2KHR(VkQueue _queue,
       }
    }
 
+   /* WSI signal info comes from WSI, which does 1 submit */
+   struct vk_sync *mem_sync = NULL;
+   if (submitCount == 1) {
+      const struct wsi_memory_signal_submit_info *mem_signal =
+         vk_find_struct_const(pSubmits->pNext, WSI_MEMORY_SIGNAL_SUBMIT_INFO_MESA);
+      bool signal_mem_sync = mem_signal != NULL &&
+                             mem_signal->memory != VK_NULL_HANDLE &&
+                             queue->base.device->create_sync_for_memory != NULL;
+      if (signal_mem_sync) {
+         VkResult result = queue->base.device->create_sync_for_memory(queue->base.device,
+                                                                      mem_signal->memory,
+                                                                      true, &mem_sync);
+         if (unlikely(result != VK_SUCCESS))
+            return result;
+      }
+   }
+
+   uint32_t prev_perf_pass_index = 0;
+   bool iterate = true;
+   bool has_perf_info = false;
+   bool has_signals = false;
+   bool needs_last = false;
+   uint32_t first = 0, last = 0;
+   uint32_t wait_count = 0, cmdbuf_count = 0;
    for (uint32_t i = 0; i < submitCount; i++) {
-      struct vulkan_submit_info info = {
-         .pNext = pSubmits[i].pNext,
-         .command_buffer_count = pSubmits[i].commandBufferInfoCount,
-         .command_buffers = pSubmits[i].pCommandBufferInfos,
-         .wait_count = pSubmits[i].waitSemaphoreInfoCount,
-         .waits = pSubmits[i].pWaitSemaphoreInfos,
-         .signal_count = pSubmits[i].signalSemaphoreInfoCount,
-         .signals = pSubmits[i].pSignalSemaphoreInfos,
-         .fence = i == submitCount - 1 ? fence : NULL
-      };
-      VkResult result = vk_queue_submit(queue, &info);
-      if (unlikely(result != VK_SUCCESS))
-         return result;
+      /* From the Vulkan 1.2.194 spec:
+      *
+      *    "If the VkSubmitInfo::pNext chain does not include this structure,
+      *    the batch defaults to use counter pass index 0."
+      */
+      const VkPerformanceQuerySubmitInfoKHR *perf_info =
+         vk_find_struct_const(pSubmits[i].pNext, PERFORMANCE_QUERY_SUBMIT_INFO_KHR);
+      uint32_t perf_pass_index = perf_info ? perf_info->counterPassIndex : 0;
+
+      /* determine when to split the submits
+       * - split if unhandled pNext is in chain
+       * - split if perf counterPassIndex changes or is added/omitted
+       * - split if signal ordering would be disrupted
+       */
+      if (!filter_pnexts(pSubmits[i].pNext))
+         iterate = false;
+      if (i && (!!perf_info != has_perf_info || (has_perf_info && perf_pass_index != prev_perf_pass_index)))
+         iterate = false;
+      if (has_signals)
+         iterate = false;
+      if (i == submitCount - 1) {
+         /* always flush on last submit*/
+         if (iterate || !i) {
+            /* include last submit for flush if it can be included */
+            wait_count += pSubmits[i].waitSemaphoreInfoCount;
+            cmdbuf_count += pSubmits[i].commandBufferInfoCount;
+            last = i;
+         } else {
+            needs_last = true;
+         }
+         iterate = false;
+      }
+
+      if (!iterate) {
+         /* submits must split: flush pending but NOT current (unless last submit) */
+         VkResult result = vk_queue_submit_flush(queue, &pSubmits[first], last - first + 1,
+                                                 wait_count,
+                                                 cmdbuf_count,
+                                                 perf_pass_index, mem_sync, i == submitCount - 1 ? fence : NULL);
+         if (unlikely(result != VK_SUCCESS))
+            return result;
+         wait_count = 0;
+         cmdbuf_count = 0;
+         first = last = i;
+         iterate = true;
+      }
+
+      /* always keep accumulating */
+      wait_count += pSubmits[i].waitSemaphoreInfoCount;
+      cmdbuf_count += pSubmits[i].commandBufferInfoCount;
+      last = i;
+
+      has_perf_info = perf_info != NULL;
+      prev_perf_pass_index = perf_pass_index;
+      has_signals = pSubmits[i].signalSemaphoreInfoCount > 0;
+      if (needs_last) {
+         /* catch the last submit if it couldn't be merged above */
+         assert(first == last);
+         assert(first == submitCount - 1);
+         VkResult result = vk_queue_submit_flush(queue, &pSubmits[first], last - first + 1,
+                                                 wait_count,
+                                                 cmdbuf_count,
+                                                 perf_pass_index, mem_sync, i == submitCount - 1 ? fence : NULL);
+         if (unlikely(result != VK_SUCCESS))
+            return result;
+      }
    }
 
    return VK_SUCCESS;
@@ -1192,6 +1313,7 @@ vk_common_QueueBindSparse(VkQueue _queue,
    }
 
    for (uint32_t i = 0; i < bindInfoCount; i++) {
+      VkResult result = VK_SUCCESS;
       const VkTimelineSemaphoreSubmitInfo *timeline_info =
          vk_find_struct_const(pBindInfo[i].pNext, TIMELINE_SEMAPHORE_SUBMIT_INFO);
       const uint64_t *wait_values = NULL;
@@ -1225,6 +1347,7 @@ vk_common_QueueBindSparse(VkQueue _queue,
          signal_values = timeline_info->pSignalSemaphoreValues;
       }
 
+      uint32_t wait_count = pBindInfo[i].waitSemaphoreCount;
       STACK_ARRAY(VkSemaphoreSubmitInfo, wait_semaphore_infos,
                   pBindInfo[i].waitSemaphoreCount);
       STACK_ARRAY(VkSemaphoreSubmitInfo, signal_semaphore_infos,
@@ -1253,8 +1376,6 @@ vk_common_QueueBindSparse(VkQueue _queue,
       }
       struct vulkan_submit_info info = {
          .pNext = pBindInfo[i].pNext,
-         .wait_count = pBindInfo[i].waitSemaphoreCount,
-         .waits = wait_semaphore_infos,
          .signal_count = pBindInfo[i].signalSemaphoreCount,
          .signals = signal_semaphore_infos,
          .buffer_bind_count = pBindInfo[i].bufferBindCount,
@@ -1265,8 +1386,51 @@ vk_common_QueueBindSparse(VkQueue _queue,
          .image_binds = pBindInfo[i].pImageBinds,
          .fence = i == bindInfoCount - 1 ? fence : NULL
       };
-      VkResult result = vk_queue_submit(queue, &info);
+      uint32_t sparse_memory_bind_entry_count = 0;
+      uint32_t sparse_memory_image_bind_entry_count = 0;
+
+      for (uint32_t i = 0; i < info.buffer_bind_count; ++i)
+         sparse_memory_bind_entry_count += info.buffer_binds[i].bindCount;
+
+      for (uint32_t i = 0; i < info.image_opaque_bind_count; ++i)
+         sparse_memory_bind_entry_count += info.image_opaque_binds[i].bindCount;
 
+      for (uint32_t i = 0; i < info.image_bind_count; ++i)
+         sparse_memory_image_bind_entry_count += info.image_binds[i].bindCount;
+
+      VkSparseMemoryBind *sparse_memory_bind_entries = NULL;
+      VkSparseImageMemoryBind *sparse_memory_image_bind_entries = NULL;
+
+      struct vk_queue_submit *submit =
+         vk_queue_submit_alloc(queue, pBindInfo[i].waitSemaphoreCount,
+                               0,
+                               info.buffer_bind_count,
+                               info.image_opaque_bind_count,
+                               info.image_bind_count,
+                               sparse_memory_bind_entry_count,
+                               sparse_memory_image_bind_entry_count,
+                               info.signal_count +
+                               (info.fence != NULL),
+                               &sparse_memory_bind_entries,
+                               &sparse_memory_image_bind_entries);
+      if (unlikely(submit == NULL))
+         return vk_error(queue, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+      bool has_binary_permanent_semaphore_wait = vk_queue_parse_waits(queue->base.device, wait_count, wait_semaphore_infos, 0, submit);
+
+      if (has_binary_permanent_semaphore_wait && queue->submit.mode == VK_QUEUE_SUBMIT_MODE_THREADED) {
+         result = vk_queue_handle_threaded_waits(queue, wait_count, wait_semaphore_infos, 0, submit);
+         if (unlikely(result != VK_SUCCESS)) {
+            vk_queue_submit_destroy(queue, submit);
+            goto fail;
+         }
+      }
+
+      result = vk_queue_submit(queue, &info, submit, 0, NULL,
+                               has_binary_permanent_semaphore_wait,
+                               sparse_memory_bind_entries,
+                               sparse_memory_image_bind_entries);
+fail:
       STACK_ARRAY_FINISH(wait_semaphore_infos);
       STACK_ARRAY_FINISH(signal_semaphore_infos);
 
